"""
Enhanced Multi-Modal DMAE Training Pipeline - ä¿®å¤ç‰ˆ
ä¿®å¤å†…å®¹: è§£å†³å­¦ç”Ÿæ¨¡å‹åˆå§‹åŒ–å’Œoptimizerä¸ºNoneçš„é—®é¢˜
"""

import os
import sys
import argparse
import yaml
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader
from torch.utils.tensorboard import SummaryWriter
import numpy as np
from tqdm import tqdm
import time
import re
from pathlib import Path

# æ·»åŠ è·¯å¾„
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from models import TeacherModel, StudentModel
from losses import CombinedLoss, MAELoss
from data_processing import CSIPreprocessor, SkeletonPreprocessor
from utils import (
    LossTracker, save_checkpoint, load_checkpoint,
    calculate_skeleton_metrics, visualize_skeleton_prediction,
    visualize_training_curves, save_training_log, load_training_log,
    print_model_info, set_seed, get_device, warmup_lr_scheduler
)

# å¯¼å…¥MMFiæ•°æ®é›†
from mmfi_dataloader import create_enhanced_mmfi_dataloaders


def get_latest_checkpoint(checkpoint_dir, prefix="teacher"):
    """ä»æ£€æŸ¥ç‚¹ç›®å½•ä¸­æ‰¾åˆ°æœ€æ–°çš„æ£€æŸ¥ç‚¹æ–‡ä»¶"""
    checkpoint_dir = Path(checkpoint_dir)

    if not checkpoint_dir.exists():
        return None, 0

    checkpoint_files = list(checkpoint_dir.glob(f"{prefix}_epoch_*.pth"))

    if not checkpoint_files:
        return None, 0

    epoch_pattern = re.compile(rf'{prefix}_epoch_(\d+)\.pth')
    checkpoints = []

    for ckpt_file in checkpoint_files:
        match = epoch_pattern.search(ckpt_file.name)
        if match:
            epoch_num = int(match.group(1))
            checkpoints.append((epoch_num, ckpt_file))

    if not checkpoints:
        return None, 0

    checkpoints.sort(key=lambda x: x[0], reverse=True)
    latest_epoch, latest_checkpoint = checkpoints[0]

    print(f"æ‰¾åˆ°æœ€æ–°æ£€æŸ¥ç‚¹: {latest_checkpoint.name} (å·²å®Œæˆ Epoch {latest_epoch})")

    return str(latest_checkpoint), latest_epoch


class EnhancedDMAETrainer:
    """å¢å¼ºå‹å¤šæ¨¡æ€DMAEè®­ç»ƒå™¨"""

    def __init__(self, config, resume_teacher=None, resume_student=None):
        self.config = config
        self.device = get_device()
        set_seed(config.get('seed', 42))

        # åˆå§‹åŒ–æ•°æ®é¢„å¤„ç†å™¨
        self.csi_preprocessor = CSIPreprocessor(**config['csi_preprocessor'])
        self.skeleton_preprocessor = SkeletonPreprocessor(**config['skeleton_preprocessor'])

        # åˆå§‹åŒ–æ•™å¸ˆæ¨¡å‹
        self.teacher_model = TeacherModel(**config['teacher_model']).to(self.device)

        # å­¦ç”Ÿæ¨¡å‹å»¶è¿Ÿåˆå§‹åŒ–
        self.student_model = None
        self.student_model_config = config['student_model']
        print("âš ï¸  å­¦ç”Ÿæ¨¡å‹å°†åœ¨ç¬¬ä¸€æ¬¡ä½¿ç”¨æ—¶åˆå§‹åŒ–ï¼ˆç­‰å¾… num_patches ç¡®å®šï¼‰")

        # æ‰“å°æ¨¡å‹ä¿¡æ¯
        print_model_info(self.teacher_model, "Teacher Model")

        # åˆå§‹åŒ–æŸå¤±å‡½æ•°
        self.teacher_loss_fn = MAELoss(**config.get('teacher_loss', {}))
        self.combined_loss_fn = CombinedLoss(**config.get('combined_loss', {}))

        # åˆå§‹åŒ–ä¼˜åŒ–å™¨
        self.teacher_optimizer = self._create_optimizer(
            self.teacher_model, config.get('teacher_optimizer', {})
        )
        self.student_optimizer = None
        self.student_optimizer_config = config.get('student_optimizer', {})

        # åˆå§‹åŒ–å­¦ä¹ ç‡è°ƒåº¦å™¨
        self.teacher_scheduler = self._create_scheduler(
            self.teacher_optimizer, config.get('teacher_scheduler', {})
        )
        self.student_scheduler = None
        self.student_scheduler_config = config.get('student_scheduler', {})

        # è®­ç»ƒçŠ¶æ€
        self.teacher_start_epoch = 0
        self.student_start_epoch = 0
        self.current_epoch = 0
        self.best_loss = float('inf')
        self.loss_tracker = LossTracker()

        # è¾“å‡ºç›®å½•
        self.output_dir = config.get('output_dir', './outputs')
        os.makedirs(self.output_dir, exist_ok=True)

        # æ—¥å¿—æ–‡ä»¶
        self.log_file = os.path.join(self.output_dir, 'training_log.json')
        self.training_log = load_training_log(self.log_file)

        # TensorBoardè®¾ç½®
        self.use_tensorboard = config.get('logging', {}).get('tensorboard', False)
        if self.use_tensorboard:
            self.tb_log_dir = os.path.join(self.output_dir, 'tensorboard_logs')
            os.makedirs(self.tb_log_dir, exist_ok=True)
            self.writer = SummaryWriter(self.tb_log_dir)
            print(f"âœ… TensorBoardæ—¥å¿—ç›®å½•: {self.tb_log_dir}")
        else:
            self.writer = None

        # æ¢å¤è®­ç»ƒ
        if resume_teacher:
            self._resume_teacher_training(resume_teacher)

        if resume_student:
            self._resume_student_training(resume_student)

    def _initialize_student_model(self):
        """åˆå§‹åŒ–å­¦ç”Ÿæ¨¡å‹ï¼ˆåœ¨ç¬¬ä¸€æ¬¡ä½¿ç”¨æ—¶è°ƒç”¨ï¼‰"""
        if self.student_model is not None:
            return

        if self.csi_preprocessor.num_patches is None:
            raise ValueError("å¿…é¡»å…ˆè¿è¡Œä¸€æ¬¡CSIé¢„å¤„ç†å™¨æ¥ç¡®å®š num_patches")

        print("\n" + "=" * 60)
        print("ğŸ”§ æ­£åœ¨åˆå§‹åŒ–å­¦ç”Ÿæ¨¡å‹...")
        print("=" * 60)

        self.student_model = StudentModel(
            num_patches=self.csi_preprocessor.num_patches,
            patch_dim=self.csi_preprocessor.patch_dim,
            **self.student_model_config
        ).to(self.device)

        print_model_info(self.student_model, "Student Model")

        # åˆ›å»ºä¼˜åŒ–å™¨
        self.student_optimizer = self._create_optimizer(
            self.student_model, self.student_optimizer_config
        )

        # åˆ›å»ºå­¦ä¹ ç‡è°ƒåº¦å™¨
        self.student_scheduler = self._create_scheduler(
            self.student_optimizer, self.student_scheduler_config
        )

        print("âœ… å­¦ç”Ÿæ¨¡å‹åˆå§‹åŒ–å®Œæˆ")
        print("=" * 60 + "\n")

    def _resume_teacher_training(self, checkpoint_path):
        """æ¢å¤æ•™å¸ˆæ¨¡å‹è®­ç»ƒ"""
        if os.path.exists(checkpoint_path):
            print(f"\næ­£åœ¨æ¢å¤æ•™å¸ˆæ¨¡å‹è®­ç»ƒ...")
            checkpoint = torch.load(checkpoint_path, map_location=self.device)

            self.teacher_model.load_state_dict(checkpoint['model_state_dict'])
            self.teacher_optimizer.load_state_dict(checkpoint['optimizer_state_dict'])

            if 'scheduler_state_dict' in checkpoint and self.teacher_scheduler:
                self.teacher_scheduler.load_state_dict(checkpoint['scheduler_state_dict'])

            self.teacher_start_epoch = checkpoint.get('epoch', 0)
            self.best_loss = checkpoint.get('loss', float('inf'))

            print(f"âœ… æ•™å¸ˆæ¨¡å‹å·²æ¢å¤: å·²å®Œæˆ Epoch {self.teacher_start_epoch}")
            print(f"   å°†ä» Epoch {self.teacher_start_epoch + 1} ç»§ç»­è®­ç»ƒ")
        else:
            print(f"âŒ æ£€æŸ¥ç‚¹æ–‡ä»¶ä¸å­˜åœ¨: {checkpoint_path}")

    def _resume_student_training(self, checkpoint_path):
        """æ¢å¤å­¦ç”Ÿæ¨¡å‹è®­ç»ƒ"""
        if os.path.exists(checkpoint_path):
            print(f"\næ­£åœ¨æ¢å¤å­¦ç”Ÿæ¨¡å‹è®­ç»ƒ...")
            checkpoint = torch.load(checkpoint_path, map_location=self.device)

            if self.student_model is None:
                print("âš ï¸  å­¦ç”Ÿæ¨¡å‹å°šæœªåˆå§‹åŒ–ï¼Œæ­£åœ¨ä»æ£€æŸ¥ç‚¹æ¢å¤...")
                self.student_start_epoch = checkpoint.get('epoch', 0)
                self.best_loss = checkpoint.get('loss', float('inf'))
                print(f"âœ… å­¦ç”Ÿæ¨¡å‹æ£€æŸ¥ç‚¹ä¿¡æ¯å·²è®°å½•: å·²å®Œæˆ Epoch {self.student_start_epoch}")
                print(f"   æ¨¡å‹å°†åœ¨è®­ç»ƒå¼€å§‹æ—¶åŠ è½½")
                return

            self.student_model.load_state_dict(checkpoint['model_state_dict'])
            self.student_optimizer.load_state_dict(checkpoint['optimizer_state_dict'])

            if 'scheduler_state_dict' in checkpoint and self.student_scheduler:
                self.student_scheduler.load_state_dict(checkpoint['scheduler_state_dict'])

            self.student_start_epoch = checkpoint.get('epoch', 0)
            self.best_loss = checkpoint.get('loss', float('inf'))

            print(f"âœ… å­¦ç”Ÿæ¨¡å‹å·²æ¢å¤: å·²å®Œæˆ Epoch {self.student_start_epoch}")
            print(f"   å°†ä» Epoch {self.student_start_epoch + 1} ç»§ç»­è®­ç»ƒ")
        else:
            print(f"âŒ æ£€æŸ¥ç‚¹æ–‡ä»¶ä¸å­˜åœ¨: {checkpoint_path}")

    def _create_optimizer(self, model, optimizer_config):
        """åˆ›å»ºä¼˜åŒ–å™¨"""
        optimizer_type = optimizer_config.get('type', 'adamw')
        lr = optimizer_config.get('lr', 1e-4)
        weight_decay = optimizer_config.get('weight_decay', 1e-2)

        if optimizer_type.lower() == 'adamw':
            optimizer = optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)
        elif optimizer_type.lower() == 'adam':
            optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)
        elif optimizer_type.lower() == 'sgd':
            momentum = optimizer_config.get('momentum', 0.9)
            optimizer = optim.SGD(model.parameters(), lr=lr, weight_decay=weight_decay, momentum=momentum)
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„ä¼˜åŒ–å™¨ç±»å‹: {optimizer_type}")

        return optimizer

    def _create_scheduler(self, optimizer, scheduler_config):
        """åˆ›å»ºå­¦ä¹ ç‡è°ƒåº¦å™¨"""
        scheduler_type = scheduler_config.get('type', 'cosine')

        if scheduler_type.lower() == 'cosine':
            T_max = scheduler_config.get('T_max', 100)
            scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=T_max)
        elif scheduler_type.lower() == 'step':
            step_size = scheduler_config.get('step_size', 30)
            gamma = scheduler_config.get('gamma', 0.1)
            scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=step_size, gamma=gamma)
        elif scheduler_type.lower() == 'warmup':
            warmup_epochs = scheduler_config.get('warmup_epochs', 10)
            total_epochs = scheduler_config.get('total_epochs', 100)
            base_lr = scheduler_config.get('base_lr', 1e-4)
            scheduler = warmup_lr_scheduler(optimizer, warmup_epochs, total_epochs, base_lr)
        else:
            scheduler = None

        return scheduler

    def load_data(self, dataset_root, config_file):
        """åŠ è½½æ•°æ®"""
        print("æ­£åœ¨åŠ è½½æ•°æ®...")

        self.train_loader, self.val_loader = create_enhanced_mmfi_dataloaders(
            dataset_root,
            config_file,
            batch_size=self.config.get('batch_size', 32),
            num_workers=self.config.get('num_workers', 4)
        )

        print(f"âœ… æ•°æ®åŠ è½½å®Œæˆ")
        print(f"   è®­ç»ƒæ‰¹æ¬¡æ•°: {len(self.train_loader)}")
        print(f"   éªŒè¯æ‰¹æ¬¡æ•°: {len(self.val_loader)}")

        print("\néªŒè¯æ•°æ®æ ¼å¼...")
        try:
            sample_batch = next(iter(self.train_loader))
            print(f"   æ‰¹æ¬¡é”®: {sample_batch.keys()}")
            print(f"   CSIå½¢çŠ¶: {sample_batch['csi_data'].shape}")
            print(f"   éª¨éª¼ç‚¹å½¢çŠ¶: {sample_batch['rgb_skeleton'].shape}")
            print("âœ… æ•°æ®æ ¼å¼éªŒè¯é€šè¿‡")
        except Exception as e:
            print(f"âŒ æ•°æ®æ ¼å¼éªŒè¯å¤±è´¥: {e}")
            raise

    def train_student_distillation(self, num_epochs):
        """é˜¶æ®µ2: å­¦ç”Ÿæ¨¡å‹è’¸é¦è®­ç»ƒ"""
        print("\n" + "=" * 60)
        print("é˜¶æ®µ2: å­¦ç”Ÿæ¨¡å‹è’¸é¦è®­ç»ƒ")
        print("=" * 60)

        # âœ… å…³é”®ä¿®å¤ï¼šç¡®ä¿å­¦ç”Ÿæ¨¡å‹å·²åˆå§‹åŒ–
        if self.student_model is None:
            print("ğŸ” å­¦ç”Ÿæ¨¡å‹å°šæœªåˆå§‹åŒ–ï¼Œæ­£åœ¨åˆå§‹åŒ–...")

            if not hasattr(self, 'train_loader') or self.train_loader is None:
                raise RuntimeError("æ•°æ®åŠ è½½å™¨æœªåˆå§‹åŒ–ï¼è¯·å…ˆè°ƒç”¨ load_data() æ–¹æ³•ã€‚")

            try:
                print("   æ­£åœ¨è·å–æ ·æœ¬æ‰¹æ¬¡...")
                sample_batch = next(iter(self.train_loader))
                csi_sample = sample_batch['csi_data'].to(self.device)
                print(f"   CSIæ ·æœ¬å½¢çŠ¶: {csi_sample.shape}")

                # æ£€æµ‹æ•°æ®æ ¼å¼å¹¶è°ƒæ•´
                if csi_sample.shape[1] == 3:
                    csi_sample_reshaped = csi_sample[:1]
                    print(f"   æ£€æµ‹åˆ°æ ‡å‡†æ ¼å¼: {csi_sample_reshaped.shape}")
                elif csi_sample.shape[-1] == 3:
                    csi_sample_reshaped = csi_sample.permute(0, 3, 1, 2)[:1]
                    print(f"   æ£€æµ‹åˆ°MMFiæ ¼å¼ï¼Œå·²è½¬æ¢: {csi_sample_reshaped.shape}")
                else:
                    raise ValueError(f"æ— æ³•è¯†åˆ«çš„CSIæ•°æ®æ ¼å¼: {csi_sample.shape}")

                print("   è¿è¡ŒCSIé¢„å¤„ç†å™¨...")
                patches, _ = self.csi_preprocessor(csi_sample_reshaped)

                if self.csi_preprocessor.num_patches is None:
                    raise RuntimeError(f"CSIé¢„å¤„ç†å™¨æœªèƒ½ç¡®å®š num_patchesï¼patches shape: {patches.shape}")

                print(f"   âœ… num_patches å·²ç¡®å®š: {self.csi_preprocessor.num_patches}")

                # åˆå§‹åŒ–å­¦ç”Ÿæ¨¡å‹
                self._initialize_student_model()
                print("   âœ… å­¦ç”Ÿæ¨¡å‹åˆå§‹åŒ–å®Œæˆ")

                # === GPT5 FIX START: è®°å½•é¢„æœŸpatchæ•°ï¼Œç¡®ä¿ä¸€è‡´ ===
                self.student_model.init_grid = self.csi_preprocessor.patch_grid
                self.student_model.expected_grid = self.csi_preprocessor.patch_grid
                self.expected_num_patches = self.csi_preprocessor.num_patches

                print(f"âœ… Student expected patch grid: {self.expected_num_patches} "
                      f"({self.csi_preprocessor.patch_grid[0]}Ã—{self.csi_preprocessor.patch_grid[1]}Ã—{self.csi_preprocessor.num_antennas})")
                # === GPT5 FIX END ===



            except StopIteration:
                raise RuntimeError("æ•°æ®åŠ è½½å™¨ä¸ºç©ºï¼è¯·æ£€æŸ¥æ•°æ®é›†æ˜¯å¦æ­£ç¡®åŠ è½½ã€‚")
            except Exception as e:
                print(f"   âŒ åˆå§‹åŒ–å¤±è´¥: {e}")
                import traceback
                traceback.print_exc()
                raise RuntimeError(
                    f"å­¦ç”Ÿæ¨¡å‹åˆå§‹åŒ–å¤±è´¥: {e}\n"
                    "å¯èƒ½çš„åŸå› :\n"
                    "1. CSIæ•°æ®æ ¼å¼ä¸åŒ¹é…\n"
                    "2. config.yaml ä¸­çš„ csi_preprocessor å‚æ•°é…ç½®é”™è¯¯\n"
                    "3. æ•°æ®åŠ è½½å™¨è¿”å›çš„æ•°æ®æ ¼å¼å¼‚å¸¸"
                ) from e

        # å†»ç»“æ•™å¸ˆæ¨¡å‹
        self.teacher_model.eval()
        for param in self.teacher_model.parameters():
            param.requires_grad = False

        self.student_model.train()

        # ä»æ¢å¤çš„epochå¼€å§‹
        start_epoch = self.student_start_epoch
        total_epochs = start_epoch + num_epochs

        print(f"è®­ç»ƒèŒƒå›´: Epoch {start_epoch + 1} åˆ° Epoch {total_epochs}")

        for epoch in range(start_epoch, total_epochs):
            actual_epoch = epoch + 1

            epoch_start_time = time.time()
            self.loss_tracker.reset_current()

            # è®­ç»ƒå¾ªç¯
            train_bar = tqdm(
                self.train_loader,
                desc=f'Epoch {actual_epoch}/{total_epochs}',
                ncols=120,
                leave=True,
                bar_format='{l_bar}{bar}| {n_fmt}/{total_fmt} [{elapsed}<{remaining}, {rate_fmt}] {postfix}'
            )

            for batch_idx, batch in enumerate(train_bar):
                # è·å–æ•°æ®
                csi_data = batch['csi_data'].to(self.device)
                rgb_skeleton = batch['rgb_skeleton'].to(self.device)

                # é¢„å¤„ç†CSIæ•°æ®
                batch_size = csi_data.shape[0]
                csi_data_reshaped = csi_data.permute(0, 3, 1, 2)
                csi_patches, _ = self.csi_preprocessor(csi_data_reshaped)

                # é¢„å¤„ç†RGBéª¨éª¼ç‚¹
                rgb_skeleton = self.skeleton_preprocessor(rgb_skeleton)

                # å¯¹æ¯”å­¦ä¹ æ ‡ç­¾
                contrast_labels = torch.ones(batch_size, device=self.device)

                # å­¦ç”Ÿæ¨¡å‹å‰å‘ä¼ æ’­
                self.student_optimizer.zero_grad()
                student_outputs = self.student_model(csi_patches)

                # æ•™å¸ˆæ¨¡å‹å‰å‘ä¼ æ’­
                with torch.no_grad():
                    teacher_features = self.teacher_model.forward_features(rgb_skeleton, mask_ratio=0.0)

                # è®¡ç®—ç»„åˆæŸå¤±
                total_loss, loss_dict = self.combined_loss_fn(
                    student_outputs['reconstructed_patches'], csi_patches, student_outputs['mask'],
                    student_outputs['skeleton_pred'], rgb_skeleton,
                    student_outputs['distill_features'], teacher_features,
                    student_outputs['contrast_features'][:batch_size//2] if batch_size > 1 else student_outputs['contrast_features'][:1],
                    student_outputs['contrast_features'][batch_size//2:] if batch_size > 1 else student_outputs['contrast_features'][:1],
                    contrast_labels[:batch_size//2] if batch_size > 1 else contrast_labels[:1]
                )

                # åå‘ä¼ æ’­
                total_loss.backward()
                self.student_optimizer.step()

                # æ›´æ–°æŸå¤±
                self.loss_tracker.update(loss_dict, csi_patches.shape[0])

                # æ›´æ–°è¿›åº¦æ¡
                current_losses = self.loss_tracker.get_current_losses()
                train_bar.set_postfix({
                    'Loss': f"{current_losses.get('total_loss', 0):.4f}",
                    'MAE': f"{current_losses.get('mae_total_loss', 0):.4f}",
                    'Distill': f"{current_losses.get('distill_loss', 0):.4f}",
                    'Contrast': f"{current_losses.get('contrast_loss', 0):.4f}"
                })

            # å­¦ä¹ ç‡è°ƒåº¦
            if self.student_scheduler:
                self.student_scheduler.step()

            # éªŒè¯
            val_loss, val_metrics = self.validate_student()

            # ä¿å­˜epochæŸå¤±
            self.loss_tracker.current_losses['student_val_loss'].update(val_loss)
            # ä¿å­˜epochæŸå¤±
            self.loss_tracker.current_losses['student_val_loss'].update(val_loss)

            # å°† val_metrics ä¸­çš„æ¯é¡¹è½¬æ¢ä¸ºå¯ä¾› AverageMeter.update() æ¥å—çš„å•ä¸€æ ‡é‡ï¼ˆå–å‡å€¼ï¼‰
            for key, value in val_metrics.items():
                # é»˜è®¤è·³è¿‡ä¸èƒ½å¤„ç†çš„ç±»å‹
                scalar_value = None

                # list -> å–å‡å€¼ï¼ˆé€‚ç”¨äº joint_errors åˆ—è¡¨ï¼‰
                if isinstance(value, (list, tuple)):
                    try:
                        scalar_value = float(np.mean(value))
                    except Exception:
                        # å¦‚æœåˆ—è¡¨å†…å«éæ•°å€¼ï¼Œè·³è¿‡è®°å½•
                        continue

                # torch.Tensor -> æ ‡é‡æˆ–å‡å€¼
                elif isinstance(value, torch.Tensor):
                    if value.numel() == 1:
                        scalar_value = float(value.item())
                    else:
                        scalar_value = float(value.mean().item())

                # ç›´æ¥æ˜¯æ•°å€¼
                elif isinstance(value, (float, int, np.floating, np.integer)):
                    scalar_value = float(value)

                # å…¶ä»–ç±»å‹è·³è¿‡
                else:
                    continue

                # æ›´æ–°åˆ° loss_trackerï¼ˆAverageMeter æœŸæœ›ä¸€ä¸ªæ•°å€¼ï¼‰
                self.loss_tracker.current_losses[f'val_{key}'].update(scalar_value)

            # ï¼ˆå¯é€‰ï¼‰æŠŠ PCK å•ç‹¬å†™å…¥ TensorBoardï¼ˆåŸæ¥é€»è¾‘ï¼‰
            for key, value in val_metrics.items():
                if self.writer and key.startswith('PCK@'):
                    # ç¡®ä¿ä¼ å…¥çš„æ˜¯æ ‡é‡
                    if isinstance(value, torch.Tensor):
                        if value.numel() == 1:
                            v = float(value.item())
                        else:
                            v = float(value.mean().item())
                    elif isinstance(value, (list, tuple)):
                        v = float(np.mean(value))
                    else:
                        v = float(value)
                    self.writer.add_scalar(f'Student/Val_{key}', v, actual_epoch)

            # TensorBoardè®°å½•
            if self.writer:
                self.writer.add_scalar('Student/Train_Total_Loss', current_losses.get('total_loss', 0), actual_epoch)
                self.writer.add_scalar('Student/Train_MAE_Loss', current_losses.get('mae_total_loss', 0), actual_epoch)
                self.writer.add_scalar('Student/Train_Distill_Loss', current_losses.get('distill_loss', 0), actual_epoch)
                self.writer.add_scalar('Student/Train_Contrast_Loss', current_losses.get('contrast_loss', 0), actual_epoch)
                self.writer.add_scalar('Student/Val_Loss', val_loss, actual_epoch)
                self.writer.add_scalar('Student/Val_MPJPE', val_metrics.get('MPJPE', 0), actual_epoch)
                self.writer.add_scalar('Student/Learning_Rate', self.student_optimizer.param_groups[0]['lr'], actual_epoch)

                for key, value in val_metrics.items():
                    if key.startswith('PCK@'):
                        self.writer.add_scalar(f'Student/Val_{key}', value, actual_epoch)

            # ä¿å­˜æ£€æŸ¥ç‚¹
            is_best = val_loss < self.best_loss
            if is_best:
                self.best_loss = val_loss

            if actual_epoch % self.config.get('save_freq', 10) == 0 or is_best:
                save_checkpoint(
                    self.student_model, self.student_optimizer, self.student_scheduler,
                    actual_epoch,
                    val_loss,
                    os.path.join(self.output_dir, 'student_checkpoints'),
                    f'student_epoch_{actual_epoch}.pth',
                    is_best
                )

            # æ‰“å°epochä¿¡æ¯
            best_marker = " ğŸ¯ Best!" if is_best else ""
            epoch_time = time.time() - epoch_start_time
            print(f"Epoch {actual_epoch}/{total_epochs} - "
                  f"Train Loss: {current_losses.get('total_loss', 0):.4f}, "
                  f"Val Loss: {val_loss:.4f}, "
                  f"Val MPJPE: {val_metrics.get('MPJPE', 0):.4f}, "
                  f"Time: {epoch_time:.2f}s{best_marker}")

        # æ›´æ–°èµ·å§‹epoch
        self.student_start_epoch = total_epochs

        print("âœ… å­¦ç”Ÿæ¨¡å‹è’¸é¦è®­ç»ƒå®Œæˆ")

    def validate_student(self):
        """éªŒè¯å­¦ç”Ÿæ¨¡å‹"""
        self.student_model.eval()
        total_loss = 0.0
        all_pred_skeletons = []
        all_target_skeletons = []
        num_batches = 0

        with torch.no_grad():
            for batch in self.val_loader:
                csi_data = batch['csi_data'].to(self.device)
                rgb_skeleton = batch['rgb_skeleton'].to(self.device)

                # é¢„å¤„ç†
                csi_data_reshaped = csi_data.permute(0, 3, 1, 2)
                csi_patches, _ = self.csi_preprocessor(csi_data_reshaped)
                rgb_skeleton = self.skeleton_preprocessor(rgb_skeleton)

                # å­¦ç”Ÿæ¨¡å‹æ¨ç†
                student_outputs = self.student_model(csi_patches, mask_ratio=0.0)

                # è®¡ç®—æŸå¤±
                skeleton_loss = nn.MSELoss()(student_outputs['skeleton_pred'], rgb_skeleton)
                total_loss += skeleton_loss.item()

                # æ”¶é›†é¢„æµ‹ç»“æœ
                all_pred_skeletons.append(student_outputs['skeleton_pred'].cpu())
                all_target_skeletons.append(rgb_skeleton.cpu())

                num_batches += 1

        self.student_model.train()

        # è®¡ç®—éª¨éª¼ç‚¹æŒ‡æ ‡
        if all_pred_skeletons:
            pred_skeletons = torch.cat(all_pred_skeletons, dim=0)
            target_skeletons = torch.cat(all_target_skeletons, dim=0)
            metrics = calculate_skeleton_metrics(pred_skeletons, target_skeletons)

            for key, value in metrics.items():
                if isinstance(value, torch.Tensor):
                    metrics[key] = value.item()
        else:
            metrics = {'MPJPE': 0.0}

        avg_loss = total_loss / num_batches if num_batches > 0 else 0.0
        return avg_loss, metrics

    def save_training_results(self):
        """ä¿å­˜è®­ç»ƒç»“æœ"""
        loss_history = self.loss_tracker.get_history()

        curves_path = os.path.join(self.output_dir, 'training_curves.png')
        visualize_training_curves(loss_history, curves_path)

        self.training_log.update({
            'loss_history': loss_history,
            'config': self.config,
            'best_loss': self.best_loss,
            'total_epochs': self.current_epoch
        })
        save_training_log(self.training_log, self.log_file)

        print(f"âœ… è®­ç»ƒç»“æœå·²ä¿å­˜åˆ°: {self.output_dir}")

    def train(self, dataset_root, config_file):
        """å®Œæ•´è®­ç»ƒæµç¨‹"""
        print("=" * 60)
        print("Enhanced Multi-Modal DMAE è®­ç»ƒå¼€å§‹")
        print("=" * 60)

        # åŠ è½½æ•°æ®
        self.load_data(dataset_root, config_file)

        # é˜¶æ®µ1: æ•™å¸ˆæ¨¡å‹é¢„è®­ç»ƒ
        teacher_epochs = self.config.get('teacher_pretrain_epochs', 50)
        if teacher_epochs > 0:
            print("âš ï¸  è·³è¿‡æ•™å¸ˆæ¨¡å‹é¢„è®­ç»ƒ (epochs=0)")

        # é˜¶æ®µ2: å­¦ç”Ÿæ¨¡å‹è’¸é¦è®­ç»ƒ
        student_epochs = self.config.get('student_distill_epochs', 100)
        if student_epochs > 0:
            self.train_student_distillation(student_epochs)

        # ä¿å­˜è®­ç»ƒç»“æœ
        self.save_training_results()

        # å…³é—­TensorBoard writer
        if self.writer:
            self.writer.close()

        print("\n" + "=" * 60)
        print("ğŸ‰ Enhanced Multi-Modal DMAE è®­ç»ƒå®Œæˆ!")
        print(f"æœ€ä½³éªŒè¯æŸå¤±: {self.best_loss:.4f}")
        print(f"è¾“å‡ºç›®å½•: {self.output_dir}")
        if self.use_tensorboard:
            print(f"TensorBoardæ—¥å¿—: {self.tb_log_dir}")
            print("å¯åŠ¨TensorBoard: tensorboard --logdir=" + self.tb_log_dir)
        print("=" * 60)


def load_config(config_path):
    """åŠ è½½é…ç½®æ–‡ä»¶"""
    with open(config_path, 'r', encoding='utf-8') as f:
        config = yaml.load(f, Loader=yaml.FullLoader)
    return config


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(
        description='Enhanced Multi-Modal DMAE Training Pipeline',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ç¤ºä¾‹:
  # ä»å¤´å¼€å§‹è®­ç»ƒ
  python train.py <dataset_root> <config_file> --config config.yaml --output_dir ./outputs

  # æ¢å¤æ•™å¸ˆæ¨¡å‹è®­ç»ƒ
  python train.py <dataset_root> <config_file> --config config.yaml --resume_teacher ./outputs/teacher_checkpoints/best_model.pth

  # æ¢å¤å­¦ç”Ÿæ¨¡å‹è®­ç»ƒ
  python train.py <dataset_root> <config_file> --config config.yaml --resume_student ./outputs/student_checkpoints/best_model.pth
        """
    )

    parser.add_argument('dataset_root', type=str, help='æ•°æ®é›†æ ¹ç›®å½•è·¯å¾„')
    parser.add_argument('config_file', type=str, help='æ•°æ®é›†é…ç½®æ–‡ä»¶è·¯å¾„ (config.yaml)')
    parser.add_argument('--config', type=str, default='config.yaml', help='è®­ç»ƒé…ç½®æ–‡ä»¶è·¯å¾„')
    parser.add_argument('--output_dir', type=str, default='./outputs', help='è¾“å‡ºç›®å½•')
    parser.add_argument('--resume_teacher', type=str, default=None, help='æ•™å¸ˆæ¨¡å‹æ£€æŸ¥ç‚¹è·¯å¾„')
    parser.add_argument('--resume_student', type=str, default=None, help='å­¦ç”Ÿæ¨¡å‹æ£€æŸ¥ç‚¹è·¯å¾„')
    parser.add_argument('--auto_resume', action='store_true', help='è‡ªåŠ¨æ¢å¤æœ€æ–°æ£€æŸ¥ç‚¹')

    args = parser.parse_args()

    print("=" * 80)
    print("Enhanced Multi-Modal DMAE Training Pipeline")
    print("=" * 80)
    print(f"æ•°æ®é›†æ ¹ç›®å½•: {args.dataset_root}")
    print(f"æ•°æ®é›†é…ç½®: {args.config_file}")
    print(f"è®­ç»ƒé…ç½®: {args.config}")
    print(f"è¾“å‡ºç›®å½•: {args.output_dir}")
    print("=" * 80)

    # éªŒè¯è·¯å¾„
    if not os.path.exists(args.dataset_root):
        print(f"âŒ é”™è¯¯: æ•°æ®é›†æ ¹ç›®å½•ä¸å­˜åœ¨: {args.dataset_root}")
        return

    if not os.path.exists(args.config_file):
        print(f"âŒ é”™è¯¯: æ•°æ®é›†é…ç½®æ–‡ä»¶ä¸å­˜åœ¨: {args.config_file}")
        return

    if not os.path.exists(args.config):
        print(f"âŒ é”™è¯¯: è®­ç»ƒé…ç½®æ–‡ä»¶ä¸å­˜åœ¨: {args.config}")
        return

    # åŠ è½½è®­ç»ƒé…ç½®
    try:
        config = load_config(args.config)
        config['output_dir'] = args.output_dir
        print("âœ… è®­ç»ƒé…ç½®åŠ è½½æˆåŠŸ")
    except Exception as e:
        print(f"âŒ é”™è¯¯: æ— æ³•åŠ è½½è®­ç»ƒé…ç½®: {e}")
        return

    # è‡ªåŠ¨æ¢å¤æ£€æŸ¥ç‚¹
    resume_teacher = args.resume_teacher
    resume_student = args.resume_student

    if args.auto_resume:
        print("\næ£€æŸ¥æ˜¯å¦å­˜åœ¨æ£€æŸ¥ç‚¹...")
        teacher_checkpoint_dir = os.path.join(args.output_dir, 'teacher_checkpoints')
        student_checkpoint_dir = os.path.join(args.output_dir, 'student_checkpoints')

        if resume_teacher is None:
            latest_teacher, _ = get_latest_checkpoint(teacher_checkpoint_dir, "teacher")
            if latest_teacher:
                resume_teacher = latest_teacher
                print(f"âœ… æ‰¾åˆ°æ•™å¸ˆæ¨¡å‹æ£€æŸ¥ç‚¹: {resume_teacher}")

        if resume_student is None:
            latest_student, _ = get_latest_checkpoint(student_checkpoint_dir, "student")
            if latest_student:
                resume_student = latest_student
                print(f"âœ… æ‰¾åˆ°å­¦ç”Ÿæ¨¡å‹æ£€æŸ¥ç‚¹: {resume_student}")

    # åˆ›å»ºè®­ç»ƒå™¨
    try:
        print("\nåˆå§‹åŒ–è®­ç»ƒå™¨...")
        trainer = EnhancedDMAETrainer(
            config,
            resume_teacher=resume_teacher,
            resume_student=resume_student
        )
        print("âœ… è®­ç»ƒå™¨åˆå§‹åŒ–æˆåŠŸ")
    except Exception as e:
        print(f"âŒ é”™è¯¯: è®­ç»ƒå™¨åˆå§‹åŒ–å¤±è´¥")
        print(f"é”™è¯¯ä¿¡æ¯: {e}")
        import traceback
        traceback.print_exc()
        return

    # å¼€å§‹è®­ç»ƒ
    try:
        print("\nå¼€å§‹è®­ç»ƒæµç¨‹...")
        trainer.train(args.dataset_root, args.config_file)
        print("\n" + "=" * 80)
        print("ğŸ‰ è®­ç»ƒå®Œæˆï¼")
        print("=" * 80)
    except KeyboardInterrupt:
        print("\nâš ï¸  è®­ç»ƒè¢«ç”¨æˆ·ä¸­æ–­")
    except Exception as e:
        print(f"\nâŒ è®­ç»ƒè¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯")
        print(f"é”™è¯¯ä¿¡æ¯: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main()