"""
Enhanced Multi-Modal DMAE Training Pipeline - æœ€ç»ˆä¼˜åŒ–ç‰ˆ

æ”¹è¿›å†…å®¹:
1. ä¿®å¤ scheduler/optimizer è°ƒç”¨é¡ºåºé—®é¢˜
2. å¯¹ç§°åŒå‘ InfoNCE å¯¹æ¯”æŸå¤±
3. å¯è®­ç»ƒçš„ teacher contrast projector
4. Contrast weight warmupæœºåˆ¶
5. è‡ªåŠ¨æ—¶é—´æˆ³æˆ–è‡ªå®šä¹‰ run_name ä¿å­˜
6. ç§»é™¤batchçº§åˆ«debugæ‰“å°
7. Epochçº§åˆ«ç»Ÿè®¡æ±‡æ€»
"""

import os
import sys
import argparse
import yaml
import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
from torch.utils.data import DataLoader
from torch.utils.tensorboard import SummaryWriter
import numpy as np
from tqdm import tqdm
import time
import re
import datetime
from pathlib import Path

# æ·»åŠ è·¯å¾„
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from models import TeacherModel, StudentModel
from losses import MAELoss, EnhancedCombinedLoss
from data_processing import CSIPreprocessor, SkeletonPreprocessor
from utils import (
    LossTracker, save_checkpoint, load_checkpoint,
    calculate_skeleton_metrics, visualize_skeleton_prediction,
    visualize_training_curves, save_training_log, load_training_log,
    print_model_info, set_seed, get_device, warmup_lr_scheduler
)

# å¯¼å…¥MMFiæ•°æ®é›†
from mmfi_dataloader import create_enhanced_mmfi_dataloaders


def get_latest_checkpoint(checkpoint_dir, prefix="teacher"):
    """ä»æ£€æŸ¥ç‚¹ç›®å½•ä¸­æ‰¾åˆ°æœ€æ–°çš„æ£€æŸ¥ç‚¹æ–‡ä»¶"""
    checkpoint_dir = Path(checkpoint_dir)

    if not checkpoint_dir.exists():
        return None, 0

    checkpoint_files = list(checkpoint_dir.glob(f"{prefix}_epoch_*.pth"))

    if not checkpoint_files:
        return None, 0

    epoch_pattern = re.compile(rf'{prefix}_epoch_(\d+)\.pth')
    checkpoints = []

    for ckpt_file in checkpoint_files:
        match = epoch_pattern.search(ckpt_file.name)
        if match:
            epoch_num = int(match.group(1))
            checkpoints.append((epoch_num, ckpt_file))

    if not checkpoints:
        return None, 0

    checkpoints.sort(key=lambda x: x[0], reverse=True)
    latest_epoch, latest_checkpoint = checkpoints[0]

    print(f"æ‰¾åˆ°æœ€æ–°æ£€æŸ¥ç‚¹: {latest_checkpoint.name} (å·²å®Œæˆ Epoch {latest_epoch})")

    return str(latest_checkpoint), latest_epoch


class EnhancedDMAETrainer:
    """å¢å¼ºå‹å¤šæ¨¡æ€DMAEè®­ç»ƒå™¨"""

    def __init__(self, config, resume_teacher=None, resume_student=None):
        self.config = config
        self.device = get_device()
        set_seed(config.get('seed', 42))

        # åˆå§‹åŒ–æ•°æ®é¢„å¤„ç†å™¨
        self.csi_preprocessor = CSIPreprocessor(**config['csi_preprocessor'])
        self.skeleton_preprocessor = SkeletonPreprocessor(**config['skeleton_preprocessor'])

        # åˆå§‹åŒ–æ•™å¸ˆæ¨¡å‹
        self.teacher_model = TeacherModel(**config['teacher_model']).to(self.device)

        # å­¦ç”Ÿæ¨¡å‹å»¶è¿Ÿåˆå§‹åŒ–
        self.student_model = None
        self.student_model_config = config['student_model']
        print("âš ï¸  å­¦ç”Ÿæ¨¡å‹å°†åœ¨ç¬¬ä¸€æ¬¡ä½¿ç”¨æ—¶åˆå§‹åŒ–(ç­‰å¾… num_patches ç¡®å®š)")

        # æ‰“å°æ¨¡å‹ä¿¡æ¯
        print_model_info(self.teacher_model, "Teacher Model")

        # åˆå§‹åŒ–æŸå¤±å‡½æ•°
        self.teacher_loss_fn = MAELoss(
            loss_type=config.get('teacher_loss', {}).get('loss_type', 'mse'),
            normalize=config.get('teacher_loss', {}).get('normalize', False)
        )
        
        combined_cfg = config.get('combined_loss', {})
        self.combined_loss_fn = EnhancedCombinedLoss(
            mae_weight=combined_cfg.get('lambda_mae', 1.0),
            distill_weight=combined_cfg.get('lambda_distill', 0.05),
            contrast_weight=combined_cfg.get('lambda_contrast', 0.1),
            contrast_temp=combined_cfg.get('contrast_temp', 0.1),
            distill_temp=combined_cfg.get('distill_temp', 1.0),
            mae_loss_config=combined_cfg.get('mae_loss_config')
        )

        # teacher->student contrast projector support
        self.teacher_contrast_projector = None
        if hasattr(self.teacher_model, 'contrast_projector'):
            print("Using teacher_model.contrast_projector (found on TeacherModel).")
            self.teacher_contrast_projector = self.teacher_model.contrast_projector
        
        # dynamic contrast warmup config
        self.contrast_target = combined_cfg.get('lambda_contrast', 0.2)
        self.contrast_warmup_epochs = combined_cfg.get('contrast_warmup_epochs', 0)

        # åˆå§‹åŒ–ä¼˜åŒ–å™¨
        self.teacher_optimizer = self._create_optimizer(
            self.teacher_model, config.get('teacher_optimizer', {})
        )
        self.student_optimizer = None
        self.student_optimizer_config = config.get('student_optimizer', {})

        # åˆå§‹åŒ–å­¦ä¹ ç‡è°ƒåº¦å™¨
        self.teacher_scheduler = self._create_scheduler(
            self.teacher_optimizer, config.get('teacher_scheduler', {})
        )
        self.student_scheduler = None
        self.student_scheduler_config = config.get('student_scheduler', {})

        # âœ… è®°å½• optimizer æ˜¯å¦ step,ç”¨äº lr_scheduler è­¦å‘Šä¿®å¤
        self._student_optimizer_stepped = False
        self._teacher_optimizer_stepped = False

        # è®­ç»ƒçŠ¶æ€
        self.teacher_start_epoch = 0
        self.student_start_epoch = 0
        self.current_epoch = 0
        self.best_loss = float('inf')
        self.loss_tracker = LossTracker()

        # âœ… è¾“å‡ºç›®å½• - æ”¯æŒæ—¶é—´æˆ³å’Œè‡ªå®šä¹‰ run_name
        base_dir = config.get('output_dir', './outputs')
        timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
        
        run_name = config.get('run_name', None)
        if not run_name:
            run_name = f"run_{timestamp}"
        self.output_dir = os.path.join(base_dir, run_name)
        
        # åˆ›å»ºå­ç›®å½•
        os.makedirs(self.output_dir, exist_ok=True)
        os.makedirs(os.path.join(self.output_dir, 'student_checkpoints'), exist_ok=True)
        os.makedirs(os.path.join(self.output_dir, 'teacher_checkpoints'), exist_ok=True)
        os.makedirs(os.path.join(self.output_dir, 'tensorboard_logs'), exist_ok=True)
        
        print(f"âœ… å½“å‰è®­ç»ƒè¾“å‡ºç›®å½•: {self.output_dir}")

        # æ—¥å¿—æ–‡ä»¶
        self.log_file = os.path.join(self.output_dir, 'training_log.json')
        self.training_log = load_training_log(self.log_file)

        # TensorBoardè®¾ç½®
        self.use_tensorboard = config.get('logging', {}).get('tensorboard', False)
        if self.use_tensorboard:
            self.tb_log_dir = os.path.join(self.output_dir, 'tensorboard_logs')
            os.makedirs(self.tb_log_dir, exist_ok=True)
            self.writer = SummaryWriter(self.tb_log_dir)
            print(f"âœ… TensorBoardæ—¥å¿—å·²å¯ç”¨: {self.tb_log_dir}")
        else:
            self.writer = None

        print("âœ… æ—¥å¿—ä¸æ£€æŸ¥ç‚¹åˆ†ç±»ä¿å­˜åˆå§‹åŒ–å®Œæ¯•")

        # æ¢å¤è®­ç»ƒ
        if resume_teacher:
            self._resume_teacher_training(resume_teacher)

        if resume_student:
            self._resume_student_training(resume_student)

    def _initialize_student_model(self):
        """åˆå§‹åŒ–å­¦ç”Ÿæ¨¡å‹(åœ¨ç¬¬ä¸€æ¬¡ä½¿ç”¨æ—¶è°ƒç”¨)"""
        if self.student_model is not None:
            return

        if self.csi_preprocessor.num_patches is None:
            raise ValueError("å¿…é¡»å…ˆè¿è¡Œä¸€æ¬¡CSIé¢„å¤„ç†å™¨æ¥ç¡®å®š num_patches")

        print("\n" + "=" * 60)
        print("ğŸ”§ æ­£åœ¨åˆå§‹åŒ–å­¦ç”Ÿæ¨¡å‹...")
        print("=" * 60)

        self.student_model = StudentModel(
            num_patches=self.csi_preprocessor.num_patches,
            patch_dim=self.csi_preprocessor.patch_dim,
            **self.student_model_config
        ).to(self.device)

        print_model_info(self.student_model, "Student Model")

        # åˆ›å»ºä¼˜åŒ–å™¨(åŒ…å«projectorå‚æ•°)
        optimizer_params = list(self.student_model.parameters())
        if self.teacher_contrast_projector is not None:
            optimizer_params += list(self.teacher_contrast_projector.parameters())
        
        self.student_optimizer = self._create_optimizer_from_params(
            optimizer_params,
            self.student_optimizer_config
        )

        # åˆ›å»ºå­¦ä¹ ç‡è°ƒåº¦å™¨
        self.student_scheduler = self._create_scheduler(
            self.student_optimizer, self.student_scheduler_config
        )

        print("âœ… å­¦ç”Ÿæ¨¡å‹åˆå§‹åŒ–å®Œæˆ")
        print("=" * 60 + "\n")

    def _resume_teacher_training(self, checkpoint_path):
        """æ¢å¤æ•™å¸ˆæ¨¡å‹è®­ç»ƒ"""
        if os.path.exists(checkpoint_path):
            print(f"\næ­£åœ¨æ¢å¤æ•™å¸ˆæ¨¡å‹è®­ç»ƒ...")
            checkpoint = torch.load(checkpoint_path, map_location=self.device)

            self.teacher_model.load_state_dict(checkpoint['model_state_dict'])
            self.teacher_optimizer.load_state_dict(checkpoint['optimizer_state_dict'])

            if 'scheduler_state_dict' in checkpoint and self.teacher_scheduler:
                self.teacher_scheduler.load_state_dict(checkpoint['scheduler_state_dict'])

            self.teacher_start_epoch = checkpoint.get('epoch', 0)
            self.best_loss = checkpoint.get('loss', float('inf'))

            print(f"âœ… æ•™å¸ˆæ¨¡å‹å·²æ¢å¤: å·²å®Œæˆ Epoch {self.teacher_start_epoch}")
            print(f"   å°†ä» Epoch {self.teacher_start_epoch + 1} ç»§ç»­è®­ç»ƒ")
        else:
            print(f"âŒ æ£€æŸ¥ç‚¹æ–‡ä»¶ä¸å­˜åœ¨: {checkpoint_path}")

    def _resume_student_training(self, checkpoint_path):
        """æ¢å¤å­¦ç”Ÿæ¨¡å‹è®­ç»ƒ"""
        if os.path.exists(checkpoint_path):
            print(f"\næ­£åœ¨æ¢å¤å­¦ç”Ÿæ¨¡å‹è®­ç»ƒ...")
            checkpoint = torch.load(checkpoint_path, map_location=self.device)

            if self.student_model is None:
                print("âš ï¸  å­¦ç”Ÿæ¨¡å‹å°šæœªåˆå§‹åŒ–,æ­£åœ¨ä»æ£€æŸ¥ç‚¹æ¢å¤...")
                self.student_start_epoch = checkpoint.get('epoch', 0)
                self.best_loss = checkpoint.get('loss', float('inf'))
                print(f"âœ… å­¦ç”Ÿæ¨¡å‹æ£€æŸ¥ç‚¹ä¿¡æ¯å·²è®°å½•: å·²å®Œæˆ Epoch {self.student_start_epoch}")
                print(f"   æ¨¡å‹å°†åœ¨è®­ç»ƒå¼€å§‹æ—¶åŠ è½½")
                return

            self.student_model.load_state_dict(checkpoint['model_state_dict'])
            self.student_optimizer.load_state_dict(checkpoint['optimizer_state_dict'])

            if 'scheduler_state_dict' in checkpoint and self.student_scheduler:
                self.student_scheduler.load_state_dict(checkpoint['scheduler_state_dict'])

            self.student_start_epoch = checkpoint.get('epoch', 0)
            self.best_loss = checkpoint.get('loss', float('inf'))

            print(f"âœ… å­¦ç”Ÿæ¨¡å‹å·²æ¢å¤: å·²å®Œæˆ Epoch {self.student_start_epoch}")
            print(f"   å°†ä» Epoch {self.student_start_epoch + 1} ç»§ç»­è®­ç»ƒ")
        else:
            print(f"âŒ æ£€æŸ¥ç‚¹æ–‡ä»¶ä¸å­˜åœ¨: {checkpoint_path}")

    def _create_optimizer(self, model, optimizer_config):
        """åˆ›å»ºä¼˜åŒ–å™¨"""
        optimizer_type = optimizer_config.get('type', 'adamw')
        lr = optimizer_config.get('lr', 1e-4)
        weight_decay = optimizer_config.get('weight_decay', 1e-2)

        if optimizer_type.lower() == 'adamw':
            optimizer = optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)
        elif optimizer_type.lower() == 'adam':
            optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)
        elif optimizer_type.lower() == 'sgd':
            momentum = optimizer_config.get('momentum', 0.9)
            optimizer = optim.SGD(model.parameters(), lr=lr, weight_decay=weight_decay, momentum=momentum)
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„ä¼˜åŒ–å™¨ç±»å‹: {optimizer_type}")

        return optimizer

    def _create_optimizer_from_params(self, params, optimizer_config):
        """åˆ›å»ºä¼˜åŒ–å™¨:ä»ç»™å®š params åˆ—è¡¨åˆ›å»º(ç”¨äºåˆå¹¶ projector)"""
        optimizer_type = optimizer_config.get('type', 'adamw')
        lr = optimizer_config.get('lr', 1e-4)
        weight_decay = optimizer_config.get('weight_decay', 1e-2)

        if optimizer_type.lower() == 'adamw':
            optimizer = optim.AdamW(params, lr=lr, weight_decay=weight_decay)
        elif optimizer_type.lower() == 'adam':
            optimizer = optim.Adam(params, lr=lr, weight_decay=weight_decay)
        elif optimizer_type.lower() == 'sgd':
            momentum = optimizer_config.get('momentum', 0.9)
            optimizer = optim.SGD(params, lr=lr, weight_decay=weight_decay, momentum=momentum)
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„ä¼˜åŒ–å™¨ç±»å‹: {optimizer_type}")

        return optimizer

    def _create_scheduler(self, optimizer, scheduler_config):
        """åˆ›å»ºå­¦ä¹ ç‡è°ƒåº¦å™¨"""
        scheduler_type = scheduler_config.get('type', 'cosine')

        if scheduler_type.lower() == 'cosine':
            T_max = scheduler_config.get('T_max', 100)
            scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=T_max)
        elif scheduler_type.lower() == 'step':
            step_size = scheduler_config.get('step_size', 30)
            gamma = scheduler_config.get('gamma', 0.1)
            scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=step_size, gamma=gamma)
        elif scheduler_type.lower() == 'warmup':
            warmup_epochs = scheduler_config.get('warmup_epochs', 10)
            total_epochs = scheduler_config.get('total_epochs', 100)
            base_lr = scheduler_config.get('base_lr', 1e-4)
            scheduler = warmup_lr_scheduler(optimizer, warmup_epochs, total_epochs, base_lr)
        else:
            scheduler = None

        return scheduler

    def load_data(self, dataset_root, config_file):
        """åŠ è½½æ•°æ®"""
        print("æ­£åœ¨åŠ è½½æ•°æ®...")

        self.train_loader, self.val_loader = create_enhanced_mmfi_dataloaders(
            dataset_root,
            config_file,
            batch_size=self.config.get('batch_size', 32),
            num_workers=self.config.get('num_workers', 4)
        )

        print(f"âœ… æ•°æ®åŠ è½½å®Œæˆ")
        print(f"   è®­ç»ƒæ‰¹æ¬¡æ•°: {len(self.train_loader)}")
        print(f"   éªŒè¯æ‰¹æ¬¡æ•°: {len(self.val_loader)}")

        print("\néªŒè¯æ•°æ®æ ¼å¼...")
        try:
            sample_batch = next(iter(self.train_loader))
            print(f"   æ‰¹æ¬¡é”®: {sample_batch.keys()}")
            print(f"   CSIå½¢çŠ¶: {sample_batch['csi_data'].shape}")
            print(f"   éª¨éª¼ç‚¹å½¢çŠ¶: {sample_batch['rgb_skeleton'].shape}")
            print("âœ… æ•°æ®æ ¼å¼éªŒè¯é€šè¿‡")
        except Exception as e:
            print(f"âŒ æ•°æ®æ ¼å¼éªŒè¯å¤±è´¥: {e}")
            raise

    def train_student_distillation(self, num_epochs):
        """é˜¶æ®µ2: å­¦ç”Ÿæ¨¡å‹è’¸é¦è®­ç»ƒ"""
        print("\n" + "=" * 60)
        print("é˜¶æ®µ2: å­¦ç”Ÿæ¨¡å‹è’¸é¦è®­ç»ƒ")
        print("=" * 60)

        # âœ… ç¡®ä¿å­¦ç”Ÿæ¨¡å‹å·²åˆå§‹åŒ–
        if self.student_model is None:
            print("ğŸ” å­¦ç”Ÿæ¨¡å‹å°šæœªåˆå§‹åŒ–,æ­£åœ¨åˆå§‹åŒ–...")

            if not hasattr(self, 'train_loader') or self.train_loader is None:
                raise RuntimeError("æ•°æ®åŠ è½½å™¨æœªåˆå§‹åŒ–!è¯·å…ˆè°ƒç”¨ load_data() æ–¹æ³•ã€‚")

            try:
                print("   æ­£åœ¨è·å–æ ·æœ¬æ‰¹æ¬¡...")
                sample_batch = next(iter(self.train_loader))
                csi_sample = sample_batch['csi_data'].to(self.device)
                print(f"   CSIæ ·æœ¬å½¢çŠ¶: {csi_sample.shape}")

                # æ£€æµ‹æ•°æ®æ ¼å¼å¹¶è°ƒæ•´
                if csi_sample.shape[1] == 3:
                    csi_sample_reshaped = csi_sample[:1]
                    print(f"   æ£€æµ‹åˆ°æ ‡å‡†æ ¼å¼: {csi_sample_reshaped.shape}")
                elif csi_sample.shape[-1] == 3:
                    csi_sample_reshaped = csi_sample.permute(0, 3, 1, 2)[:1]
                    print(f"   æ£€æµ‹åˆ°MMFiæ ¼å¼,å·²è½¬æ¢: {csi_sample_reshaped.shape}")
                else:
                    raise ValueError(f"æ— æ³•è¯†åˆ«çš„CSIæ•°æ®æ ¼å¼: {csi_sample.shape}")

                print("   è¿è¡ŒCSIé¢„å¤„ç†å™¨...")
                patches, _ = self.csi_preprocessor(csi_sample_reshaped)

                if self.csi_preprocessor.num_patches is None:
                    raise RuntimeError(f"CSIé¢„å¤„ç†å™¨æœªèƒ½ç¡®å®š num_patches! patches shape: {patches.shape}")

                print(f"   âœ… num_patches å·²ç¡®å®š: {self.csi_preprocessor.num_patches}")

                # åˆå§‹åŒ–å­¦ç”Ÿæ¨¡å‹
                self._initialize_student_model()
                print("   âœ… å­¦ç”Ÿæ¨¡å‹åˆå§‹åŒ–å®Œæˆ")

                # è®°å½•é¢„æœŸpatchæ•°
                self.student_model.init_grid = self.csi_preprocessor.patch_grid
                self.student_model.expected_grid = self.csi_preprocessor.patch_grid
                self.expected_num_patches = self.csi_preprocessor.num_patches

                print(f"âœ… Student expected patch grid: {self.expected_num_patches} "
                      f"({self.csi_preprocessor.patch_grid[0]}Ã—{self.csi_preprocessor.patch_grid[1]}Ã—{self.csi_preprocessor.num_antennas})")

            except StopIteration:
                raise RuntimeError("æ•°æ®åŠ è½½å™¨ä¸ºç©º!è¯·æ£€æŸ¥æ•°æ®é›†æ˜¯å¦æ­£ç¡®åŠ è½½ã€‚")
            except Exception as e:
                print(f"   âŒ åˆå§‹åŒ–å¤±è´¥: {e}")
                import traceback
                traceback.print_exc()
                raise RuntimeError(
                    f"å­¦ç”Ÿæ¨¡å‹åˆå§‹åŒ–å¤±è´¥: {e}\n"
                    "å¯èƒ½çš„åŸå› :\n"
                    "1. CSIæ•°æ®æ ¼å¼ä¸åŒ¹é…\n"
                    "2. config.yaml ä¸­çš„ csi_preprocessor å‚æ•°é…ç½®é”™è¯¯\n"
                    "3. æ•°æ®åŠ è½½å™¨è¿”å›çš„æ•°æ®æ ¼å¼å¼‚å¸¸"
                ) from e

        # å†»ç»“æ•™å¸ˆæ¨¡å‹
        self.teacher_model.eval()
        for param in self.teacher_model.parameters():
            param.requires_grad = False

        self.student_model.train()

        # ä»æ¢å¤çš„epochå¼€å§‹
        start_epoch = self.student_start_epoch
        total_epochs = start_epoch + num_epochs

        print(f"è®­ç»ƒèŒƒå›´: Epoch {start_epoch + 1} åˆ° Epoch {total_epochs}")

        for epoch in range(start_epoch, total_epochs):
            actual_epoch = epoch + 1

            # âœ… è®¡ç®—åŠ¨æ€ contrast weight (warmup)
            if self.contrast_warmup_epochs > 0:
                if actual_epoch <= self.contrast_warmup_epochs:
                    cur_weight = self.contrast_target * (actual_epoch / float(self.contrast_warmup_epochs))
                else:
                    cur_weight = self.contrast_target
            else:
                cur_weight = self.contrast_target
            
            # è®¾ç½®åˆ° combined_loss_fn
            if hasattr(self.combined_loss_fn, 'contrast_weight'):
                self.combined_loss_fn.contrast_weight = cur_weight

            epoch_start_time = time.time()
            self.loss_tracker.reset_current()

            # è®­ç»ƒå¾ªç¯
            train_bar = tqdm(
                self.train_loader,
                desc=f'Epoch {actual_epoch}/{total_epochs}',
                ncols=120,
                leave=True,
                bar_format='{l_bar}{bar}| {n_fmt}/{total_fmt} [{elapsed}<{remaining}, {rate_fmt}] {postfix}'
            )

            for batch_idx, batch in enumerate(train_bar):
                # è·å–æ•°æ®
                csi_data = batch['csi_data'].to(self.device)
                rgb_skeleton = batch['rgb_skeleton'].to(self.device)

                # é¢„å¤„ç†CSIæ•°æ®
                batch_size = csi_data.shape[0]
                csi_data_reshaped = csi_data.permute(0, 3, 1, 2)
                csi_patches, _ = self.csi_preprocessor(csi_data_reshaped)

                # é¢„å¤„ç†RGBéª¨éª¼ç‚¹
                rgb_skeleton = self.skeleton_preprocessor(rgb_skeleton)

                # å¯¹æ¯”å­¦ä¹ æ ‡ç­¾
                contrast_labels = torch.ones(batch_size, device=self.device)

                # å­¦ç”Ÿæ¨¡å‹å‰å‘ä¼ æ’­
                self.student_optimizer.zero_grad()
                student_outputs = self.student_model(csi_patches)

                # æ•™å¸ˆæ¨¡å‹å‰å‘ä¼ æ’­ + æå–å¯¹é½ç‰¹å¾
                with torch.no_grad():
                    teacher_features = self.teacher_model.forward_features(rgb_skeleton, mask_ratio=0.0)
                    teacher_cls = teacher_features[-1][:, 0, :]  # [B, teacher_dim]

                # âœ… contrast projector (lazyåˆ›å»º,å¯è®­ç»ƒ)
                if hasattr(self.teacher_model, 'contrast_projector'):
                    teacher_contrast = self.teacher_model.contrast_projector(teacher_cls)
                else:
                    # lazy-create projector on first batch
                    if self.teacher_contrast_projector is None:
                        student_contrast_dim = student_outputs['contrast_features'].shape[-1]
                        teacher_cls_dim = teacher_cls.shape[-1]
                        self.teacher_contrast_projector = nn.Linear(teacher_cls_dim, student_contrast_dim).to(self.device)
                        
                        # é‡å»ºoptimizeråŒ…å«projectorå‚æ•°
                        optimizer_params = list(self.student_model.parameters()) + list(self.teacher_contrast_projector.parameters())
                        self.student_optimizer = self._create_optimizer_from_params(optimizer_params, self.student_optimizer_config)
                        print(f"[Info] Created teacher_contrast_projector lazily: {teacher_cls_dim} -> {student_contrast_dim}")
                    
                    teacher_contrast = self.teacher_contrast_projector(teacher_cls)

                # âœ… normalize contrast features (ç¡®ä¿ä¼ å…¥å½’ä¸€åŒ–ç‰¹å¾)
                teacher_contrast = F.normalize(teacher_contrast, dim=-1)
                student_contrast = F.normalize(student_outputs['contrast_features'], dim=-1)

                # === è®¡ç®—ç»„åˆæŸå¤± ===
                total_loss, loss_dict = self.combined_loss_fn(
                    # MAE
                    student_outputs['reconstructed_patches'], csi_patches, student_outputs['mask'],
                    student_outputs['skeleton_pred'], rgb_skeleton,
                    # è’¸é¦
                    student_outputs['distill_features'], teacher_features,
                    # å¯¹æ¯” (ä½¿ç”¨å½’ä¸€åŒ–ç‰¹å¾)
                    student_contrast, teacher_contrast,
                    contrast_labels  # ä¿ç•™å…¼å®¹æ€§
                )

                # åå‘ä¼ æ’­
                total_loss.backward()
                self.student_optimizer.step()
                
                # âœ… æ ‡è®° optimizer.step() å·²è°ƒç”¨(ä¿®å¤ UserWarning)
                self._student_optimizer_stepped = True

                # æ›´æ–°æŸå¤±
                self.loss_tracker.update(loss_dict, csi_patches.shape[0])

                # æ›´æ–°è¿›åº¦æ¡
                current_losses = self.loss_tracker.get_current_losses()
                train_bar.set_postfix({
                    'Loss': f"{current_losses.get('total_loss', 0):.4f}",
                    'MAE': f"{current_losses.get('mae_total_loss', 0):.4f}",
                    'Distill': f"{current_losses.get('distill_loss', 0):.4f}",
                    'Contrast': f"{current_losses.get('contrast_loss', 0):.4f}"
                })

            # âœ… å­¦ä¹ ç‡è°ƒåº¦(å¿…é¡»åœ¨ optimizer.step() ä¹‹åè°ƒç”¨)
            if self.student_scheduler and self._student_optimizer_stepped:
                self.student_scheduler.step()

            # éªŒè¯
            val_loss, val_metrics = self.validate_student()

            # ä¿å­˜epochæŸå¤±
            self.loss_tracker.current_losses['student_val_loss'].update(val_loss)

            # å°† val_metrics è½¬æ¢ä¸ºæ ‡é‡
            for key, value in val_metrics.items():
                scalar_value = None

                if isinstance(value, (list, tuple)):
                    try:
                        scalar_value = float(np.mean(value))
                    except Exception:
                        continue
                elif isinstance(value, torch.Tensor):
                    if value.numel() == 1:
                        scalar_value = float(value.item())
                    else:
                        scalar_value = float(value.mean().item())
                elif isinstance(value, (float, int, np.floating, np.integer)):
                    scalar_value = float(value)
                else:
                    continue

                self.loss_tracker.current_losses[f'val_{key}'].update(scalar_value)

            # ä¿å­˜æ£€æŸ¥ç‚¹
            is_best = val_loss < self.best_loss
            if is_best:
                self.best_loss = val_loss

            if actual_epoch % self.config.get('save_freq', 10) == 0 or is_best:
                save_checkpoint(
                    self.student_model, self.student_optimizer, self.student_scheduler,
                    actual_epoch,
                    val_loss,
                    os.path.join(self.output_dir, 'student_checkpoints'),
                    f'student_epoch_{actual_epoch}.pth',
                    is_best
                )

            # âœ… --- epoch summary + contrast summary ---
            best_marker = " ğŸ¯ Best!" if is_best else ""
            epoch_time = time.time() - epoch_start_time

            # Get train losses
            train_losses = current_losses
            train_loss_val = float(train_losses.get('total_loss', 0))
            mae_val = float(train_losses.get('mae_total_loss', 0))

            # âœ… Contrast summary: ä» contrast_loss æ¨¡å—è¯»å–ç»Ÿè®¡
            contrast_module = getattr(self.combined_loss_fn, 'contrast_loss', None)
            contrast_summary_str = ""
            if contrast_module is not None and hasattr(contrast_module, 'loss_values') and len(contrast_module.loss_values) > 0:
                pos_mean = float(np.mean(contrast_module.pos_sims))
                neg_mean = float(np.mean(contrast_module.neg_sims))
                contrast_mean = float(np.mean(contrast_module.loss_values))
                contrast_summary_str = f" [Contrast] Loss={contrast_mean:.4f}, PosSim={pos_mean:.4f}, NegSim={neg_mean:.4f}"

                # å†™å…¥ TensorBoard
                if self.writer:
                    self.writer.add_scalar('Student/Contrast_PosSim', pos_mean, actual_epoch)
                    self.writer.add_scalar('Student/Contrast_NegSim', neg_mean, actual_epoch)
                    self.writer.add_scalar('Student/Contrast_Loss_Mean', contrast_mean, actual_epoch)

                # âœ… æ¸…ç©ºç»Ÿè®¡
                if hasattr(contrast_module, 'clear_stats'):
                    contrast_module.clear_stats()
                else:
                    contrast_module.pos_sims.clear()
                    contrast_module.neg_sims.clear()
                    contrast_module.loss_values.clear()

            # TensorBoard è®°å½•
            if self.writer:
                self.writer.add_scalar('Student/Train_Total_Loss', train_loss_val, actual_epoch)
                self.writer.add_scalar('Student/Train_MAE_Loss', mae_val, actual_epoch)
                self.writer.add_scalar('Student/Train_Distill_Loss', train_losses.get('distill_loss', 0), actual_epoch)
                self.writer.add_scalar('Student/Train_Contrast_Loss', train_losses.get('contrast_loss', 0), actual_epoch)
                self.writer.add_scalar('Student/Val_Loss', val_loss, actual_epoch)
                self.writer.add_scalar('Student/Val_MPJPE', val_metrics.get('MPJPE', 0), actual_epoch)
                self.writer.add_scalar('Student/Learning_Rate', self.student_optimizer.param_groups[0]['lr'], actual_epoch)
                self.writer.add_scalar('Student/Contrast_Weight', cur_weight, actual_epoch)

                # PCK metrics
                for key, value in val_metrics.items():
                    if key.startswith('PCK@'):
                        if isinstance(value, torch.Tensor):
                            if value.numel() == 1:
                                v = float(value.item())
                            else:
                                v = float(value.mean().item())
                        elif isinstance(value, (list, tuple)):
                            v = float(np.mean(value))
                        else:
                            v = float(value)
                        self.writer.add_scalar(f'Student/Val_{key}', v, actual_epoch)

            # âœ… å•è¡Œepoch summaryæ‰“å°
            print(f"Epoch {actual_epoch}/{total_epochs} - "
                  f"Train Loss: {train_loss_val:.4f}, "
                  f"Val Loss: {val_loss:.4f}, "
                  f"MPJPE: {val_metrics.get('MPJPE', 0):.4f}"
                  f"{contrast_summary_str}{best_marker} "
                  f"Time: {epoch_time:.2f}s")

        # æ›´æ–°èµ·å§‹epoch
        self.student_start_epoch = total_epochs

        print("âœ… å­¦ç”Ÿæ¨¡å‹è’¸é¦è®­ç»ƒå®Œæˆ")

    def validate_student(self):
        """éªŒè¯å­¦ç”Ÿæ¨¡å‹"""
        self.student_model.eval()
        total_loss = 0.0
        all_pred_skeletons = []
        all_target_skeletons = []
        num_batches = 0

        with torch.no_grad():
            for batch in self.val_loader:
                csi_data = batch['csi_data'].to(self.device)
                rgb_skeleton = batch['rgb_skeleton'].to(self.device)

                # é¢„å¤„ç†
                csi_data_reshaped = csi_data.permute(0, 3, 1, 2)
                csi_patches, _ = self.csi_preprocessor(csi_data_reshaped)
                rgb_skeleton = self.skeleton_preprocessor(rgb_skeleton)

                # å­¦ç”Ÿæ¨¡å‹æ¨ç†
                student_outputs = self.student_model(csi_patches, mask_ratio=0.0)

                # è®¡ç®—æŸå¤±
                skeleton_loss = nn.MSELoss()(student_outputs['skeleton_pred'], rgb_skeleton)
                total_loss += skeleton_loss.item()

                # æ”¶é›†é¢„æµ‹ç»“æœ
                all_pred_skeletons.append(student_outputs['skeleton_pred'].cpu())
                all_target_skeletons.append(rgb_skeleton.cpu())

                num_batches += 1

        self.student_model.train()

        # è®¡ç®—éª¨éª¼ç‚¹æŒ‡æ ‡
        if all_pred_skeletons:
            pred_skeletons = torch.cat(all_pred_skeletons, dim=0)
            target_skeletons = torch.cat(all_target_skeletons, dim=0)
            metrics = calculate_skeleton_metrics(pred_skeletons, target_skeletons)

            for key, value in metrics.items():
                if isinstance(value, torch.Tensor):
                    metrics[key] = value.item()
        else:
            metrics = {'MPJPE': 0.0}

        avg_loss = total_loss / num_batches if num_batches > 0 else 0.0
        return avg_loss, metrics

    def save_training_results(self):
        """ä¿å­˜è®­ç»ƒç»“æœ"""
        loss_history = self.loss_tracker.get_history()

        curves_path = os.path.join(self.output_dir, 'training_curves.png')
        visualize_training_curves(loss_history, curves_path)

        self.training_log.update({
            'loss_history': loss_history,
            'config': self.config,
            'best_loss': self.best_loss,
            'total_epochs': self.current_epoch
        })
        save_training_log(self.training_log, self.log_file)

        print(f"âœ… è®­ç»ƒç»“æœå·²ä¿å­˜åˆ°: {self.output_dir}")

    def train(self, dataset_root, config_file):
        """å®Œæ•´è®­ç»ƒæµç¨‹"""
        print("=" * 60)
        print("Enhanced Multi-Modal DMAE è®­ç»ƒå¼€å§‹")
        print("=" * 60)

        # åŠ è½½æ•°æ®
        self.load_data(dataset_root, config_file)

        # é˜¶æ®µ1: æ•™å¸ˆæ¨¡å‹é¢„è®­ç»ƒ
        teacher_epochs = self.config.get('teacher_pretrain_epochs', 50)
        if teacher_epochs > 0:
            print("âš ï¸  è·³è¿‡æ•™å¸ˆæ¨¡å‹é¢„è®­ç»ƒ (epochs=0)")

        # é˜¶æ®µ2: å­¦ç”Ÿæ¨¡å‹è’¸é¦è®­ç»ƒ
        student_epochs = self.config.get('student_distill_epochs', 100)
        if student_epochs > 0:
            self.train_student_distillation(student_epochs)

        # ä¿å­˜è®­ç»ƒç»“æœ
        self.save_training_results()

        # å…³é—­TensorBoard writer
        if self.writer:
            self.writer.close()

        print("\n" + "=" * 60)
        print("ğŸ‰ Enhanced Multi-Modal DMAE è®­ç»ƒå®Œæˆ!")
        print(f"æœ€ä½³éªŒè¯æŸå¤±: {self.best_loss:.4f}")
        print(f"è¾“å‡ºç›®å½•: {self.output_dir}")
        if self.use_tensorboard:
            print(f"TensorBoardæ—¥å¿—: {self.tb_log_dir}")
            print("å¯åŠ¨TensorBoard: tensorboard --logdir=" + self.tb_log_dir)
        print("=" * 60)


def load_config(config_path):
    """åŠ è½½é…ç½®æ–‡ä»¶"""
    with open(config_path, 'r', encoding='utf-8') as f:
        config = yaml.load(f, Loader=yaml.FullLoader)
    return config


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(
        description='Enhanced Multi-Modal DMAE Training Pipeline',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ç¤ºä¾‹:
  # ä»å¤´å¼€å§‹è®­ç»ƒ
  python train.py <dataset_root> <config_file> --config config.yaml --output_dir ./outputs

  # æ¢å¤æ•™å¸ˆæ¨¡å‹è®­ç»ƒ
  python train.py <dataset_root> <config_file> --config config.yaml --resume_teacher ./outputs/teacher_checkpoints/best_model.pth

  # æ¢å¤å­¦ç”Ÿæ¨¡å‹è®­ç»ƒ
  python train.py <dataset_root> <config_file> --config config.yaml --resume_student ./outputs/student_checkpoints/best_model.pth
  
  # è‡ªå®šä¹‰è¿è¡Œåç§°
  python train.py <dataset_root> <config_file> --config config.yaml --run_name experiment_v1
        """
    )

    parser.add_argument('dataset_root', type=str, help='æ•°æ®é›†æ ¹ç›®å½•è·¯å¾„')
    parser.add_argument('config_file', type=str, help='æ•°æ®é›†é…ç½®æ–‡ä»¶è·¯å¾„ (config.yaml)')
    parser.add_argument('--config', type=str, default='config.yaml', help='è®­ç»ƒé…ç½®æ–‡ä»¶è·¯å¾„')
    parser.add_argument('--output_dir', type=str, default='./outputs', help='è¾“å‡ºç›®å½•')
    parser.add_argument('--resume_teacher', type=str, default=None, help='æ•™å¸ˆæ¨¡å‹æ£€æŸ¥ç‚¹è·¯å¾„')
    parser.add_argument('--resume_student', type=str, default=None, help='å­¦ç”Ÿæ¨¡å‹æ£€æŸ¥ç‚¹è·¯å¾„')
    parser.add_argument('--auto_resume', action='store_true', help='è‡ªåŠ¨æ¢å¤æœ€æ–°æ£€æŸ¥ç‚¹')
    
    # âœ… æ–°å¢ï¼šå…è®¸è‡ªå®šä¹‰è¿è¡Œåç§°ï¼ˆé¿å…æ¯æ¬¡è¦†ç›–ï¼‰
    parser.add_argument('--run_name', type=str, default=None, help='è‡ªå®šä¹‰å½“å‰è®­ç»ƒ run åç§°')

    args = parser.parse_args()

    print("=" * 80)
    print("Enhanced Multi-Modal DMAE Training Pipeline")
    print("=" * 80)
    print(f"æ•°æ®é›†æ ¹ç›®å½•: {args.dataset_root}")
    print(f"æ•°æ®é›†é…ç½®: {args.config_file}")
    print(f"è®­ç»ƒé…ç½®: {args.config}")
    print(f"è¾“å‡ºç›®å½•: {args.output_dir}")
    if args.run_name:
        print(f"è¿è¡Œåç§°: {args.run_name}")
    print("=" * 80)

    # éªŒè¯è·¯å¾„
    if not os.path.exists(args.dataset_root):
        print(f"âŒ é”™è¯¯: æ•°æ®é›†æ ¹ç›®å½•ä¸å­˜åœ¨: {args.dataset_root}")
        return

    if not os.path.exists(args.config_file):
        print(f"âŒ é”™è¯¯: æ•°æ®é›†é…ç½®æ–‡ä»¶ä¸å­˜åœ¨: {args.config_file}")
        return

    if not os.path.exists(args.config):
        print(f"âŒ é”™è¯¯: è®­ç»ƒé…ç½®æ–‡ä»¶ä¸å­˜åœ¨: {args.config}")
        return

    # åŠ è½½è®­ç»ƒé…ç½®
    try:
        config = load_config(args.config)
        config['output_dir'] = args.output_dir
        
        # âœ… è®¾ç½® run_name
        if args.run_name:
            config['run_name'] = args.run_name
        
        print("âœ… è®­ç»ƒé…ç½®åŠ è½½æˆåŠŸ")
    except Exception as e:
        print(f"âŒ é”™è¯¯: æ— æ³•åŠ è½½è®­ç»ƒé…ç½®: {e}")
        return

    # è‡ªåŠ¨æ¢å¤æ£€æŸ¥ç‚¹
    resume_teacher = args.resume_teacher
    resume_student = args.resume_student

    if args.auto_resume:
        print("\næ£€æŸ¥æ˜¯å¦å­˜åœ¨æ£€æŸ¥ç‚¹...")
        # âœ… æ ¹æ® run_name ç¡®å®šæ£€æŸ¥ç‚¹ç›®å½•
        if args.run_name:
            base_checkpoint_dir = os.path.join(args.output_dir, args.run_name)
        else:
            base_checkpoint_dir = args.output_dir
            
        teacher_checkpoint_dir = os.path.join(base_checkpoint_dir, 'teacher_checkpoints')
        student_checkpoint_dir = os.path.join(base_checkpoint_dir, 'student_checkpoints')

        if resume_teacher is None:
            latest_teacher, _ = get_latest_checkpoint(teacher_checkpoint_dir, "teacher")
            if latest_teacher:
                resume_teacher = latest_teacher
                print(f"âœ… æ‰¾åˆ°æ•™å¸ˆæ¨¡å‹æ£€æŸ¥ç‚¹: {resume_teacher}")

        if resume_student is None:
            latest_student, _ = get_latest_checkpoint(student_checkpoint_dir, "student")
            if latest_student:
                resume_student = latest_student
                print(f"âœ… æ‰¾åˆ°å­¦ç”Ÿæ¨¡å‹æ£€æŸ¥ç‚¹: {resume_student}")

    # åˆ›å»ºè®­ç»ƒå™¨
    try:
        print("\nåˆå§‹åŒ–è®­ç»ƒå™¨...")
        trainer = EnhancedDMAETrainer(
            config,
            resume_teacher=resume_teacher,
            resume_student=resume_student
        )
        print("âœ… è®­ç»ƒå™¨åˆå§‹åŒ–æˆåŠŸ")
    except Exception as e:
        print(f"âŒ é”™è¯¯: è®­ç»ƒå™¨åˆå§‹åŒ–å¤±è´¥")
        print(f"é”™è¯¯ä¿¡æ¯: {e}")
        import traceback
        traceback.print_exc()
        return

    # å¼€å§‹è®­ç»ƒ
    try:
        print("\nå¼€å§‹è®­ç»ƒæµç¨‹...")
        trainer.train(args.dataset_root, args.config_file)
        print("\n" + "=" * 80)
        print("ğŸ‰ è®­ç»ƒå®Œæˆ!")
        print("=" * 80)
    except KeyboardInterrupt:
        print("\nâš ï¸  è®­ç»ƒè¢«ç”¨æˆ·ä¸­æ–­")
    except Exception as e:
        print(f"\nâŒ è®­ç»ƒè¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯")
        print(f"é”™è¯¯ä¿¡æ¯: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main()