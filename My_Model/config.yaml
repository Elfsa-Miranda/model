# Enhanced Multi-Modal DMAE Configuration - 优化版
# 增强型多模态DMAE配置文件

# 基础设置
seed: 42
output_dir: "./outputs"
batch_size: 32              # ✅ 建议 ≥16 for contrastive learning
num_workers: 4

# CSI数据预处理器配置
csi_preprocessor:
  num_antennas: 3
  num_subcarriers: 114
  time_length: 10
  stft_window: 64
  stft_hop: 16
  patch_size: 8
  normalize: true

# 骨骼点预处理器配置
skeleton_preprocessor:
  num_joints: 17
  coord_dim: 2
  normalize: true

# 教师模型配置 (RGB骨骼点MAE)
teacher_model:
  num_joints: 17
  coord_dim: 2
  embed_dim: 768
  depth: 12
  num_heads: 12
  decoder_embed_dim: 512
  decoder_depth: 8
  decoder_num_heads: 16
  mlp_ratio: 4.0
  mask_ratio: 0.75

# 学生模型配置 (CSI多分支)
student_model:
  embed_dim: 768
  depth: 12
  num_heads: 12
  decoder_embed_dim: 512
  decoder_depth: 8
  decoder_num_heads: 16
  mlp_ratio: 4.0
  num_joints: 17
  coord_dim: 2
  contrast_dim: 128         # ✅ 对比学习特征维度(降低到128)
  mask_ratio: 0.75
  num_antennas: 3
  use_multi_attn: true      # ✅ 多路径注意力开关

# ========== ✅ 增强版组合损失配置 ==========
combined_loss:
  # 损失权重(关键调优参数)
  lambda_mae: 1.0           # MAE重建损失权重
  lambda_distill: 0.05      # ✅ 蒸馏损失权重(降低,防止主导)
  lambda_contrast: 0.2      # ✅ 对比学习目标权重
  
  # ✅ 新增: Contrast weight warmup 配置
  contrast_warmup_epochs: 10  # 前 N epoch 从 0 逐步增加到 lambda_contrast

  # 温度参数
  contrast_temp: 0.1        # ✅ InfoNCE温度(0.07-0.2推荐)
  distill_temp: 1.0         # 蒸馏温度(1.0-4.0)

  # MAE损失配置
  mae_loss_config:
    loss_type: "mse"
    normalize: false

# 优化器配置
teacher_optimizer:
  type: "adamw"
  lr: 1.0e-4
  weight_decay: 1.0e-2

student_optimizer:
  type: "adamw"
  lr: 1.0e-4              # ✅ 可适当降低(5e-5)以稳定训练
  weight_decay: 1.0e-2

# 学习率调度器配置
teacher_scheduler:
  type: "cosine"
  T_max: 50

student_scheduler:
  type: "cosine"
  T_max: 100
  eta_min: 1.0e-6         # ✅ 最小学习率

# 训练配置
teacher_pretrain_epochs: 0      # 跳过教师预训练(假设已有预训练模型)
student_distill_epochs: 60      # 学生蒸馏训练epoch数
save_freq: 10                   # 保存频率

# 验证和测试配置
validation:
  freq: 1
  visualize_freq: 10
  num_vis_samples: 4

# ========== ✅ 增强日志配置 ==========
logging:
  log_freq: 50                  # ✅ 降低日志频率(减少输出)
  tensorboard: true
  wandb: false

  # ✅ 新增:对比学习可视化
  log_contrast_similarity: true  # 是否记录相似度矩阵
  log_feature_norm: true         # 是否记录特征范数

# 模型保存配置
checkpointing:
  save_best: true
  save_last: true
  save_freq: 10
  max_checkpoints: 5

# 硬件配置
hardware:
  num_workers: 4
  pin_memory: true
  mixed_precision: false        # ✅ 可尝试开启(需要PyTorch ≥1.6)

# ========== ✅ 实验跟踪配置 ==========
experiment:
  name: "enhanced_dmae_v3"
  tags: ["dmae", "csi", "rgb", "infonce", "stable_distill", "warmup", "trainable_projector"]
  notes: |
    Enhanced Multi-Modal DMAE with:
    - InfoNCE contrastive learning (one-to-one alignment)
    - Stable distillation (reduced weight + temperature)
    - Trainable teacher->student contrast projector
    - Contrast weight warmup (0 -> target over N epochs)
    - Epoch-level statistics reporting (no batch-level prints)
    
    Expected improvements:
    - Contrast loss > 0 and decreasing
    - PosSim increasing (> 0.2 mid-training, > 0.4 ideal)
    - NegSim staying low (< PosSim)
    - Distill loss stabilized (~1-5 range)
    - Faster MPJPE convergence

# ========== ✅ 调试模式 ==========
debug:
  enabled: false              # 开启后打印详细调试信息
  check_gradients: false      # 检查梯度流
  log_first_batch: true       # 记录第一个batch的统计信息